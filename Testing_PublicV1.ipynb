{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xw1Dh8YlsTi_",
        "RG-dLd-qsZ6R",
        "OlJweHPZMuVj",
        "-uvQqJMRseK8",
        "qhGUp8pcxNVx",
        "gnsUQwAUL83J",
        "Ei5Brz9j0Ao3",
        "hVttJIvcnZgz",
        "WnQw54x1ng49",
        "0ECShjKKTQ6k",
        "_lntNVdssjUK",
        "WaTSsYV-WYox"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hey! In this notebook, we are going to be testing out our models using either the DDI dataset or the SynthDerm dataset to assess bias. So, we are going to test a model over the entire dataset, as well as by aggregated fitzpatrick skin type. In this, we are going to be looking for a difference in metrics across skin types, which will indicate the existence of bias.\n",
        "\n",
        "When testing on the entire dataset, we generate the following metrics:\n",
        "\n",
        "1. Classification Report\n",
        "2. ROC-AUC\n",
        "3. Confusion Matrix\n",
        "\n",
        "When analyzing the dataset across each skin type, we will generate the following metrics:\n",
        "\n",
        "1. Accuracy\n",
        "2. ROC-AUC\n",
        "3. Classification Report\n",
        "4. F1-Scores\n",
        "5. Confusion Matrix\n",
        "\n",
        "Lastly, we will visually analyze the model to get a more in-depth view on the types of images where are model is failing (ex: never classifies images with hair correctly)."
      ],
      "metadata": {
        "id": "g7XDP-C6JNRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Public Release Notes**:\n",
        "\n",
        "1. Some code has been omitted and variables set to \"None\". This is done in the interest of privacy. If you are attempting to run this code on your own and encounter and trouble, send me an email and I will assist you however I can."
      ],
      "metadata": {
        "id": "nLrAYGvKJWux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installs and Imports**"
      ],
      "metadata": {
        "id": "xw1Dh8YlsTi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we will just import the necessary libraries before we start."
      ],
      "metadata": {
        "id": "d3KVCq9_J5Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "import shutil\n",
        "\n",
        "from glob import glob\n",
        "import io\n",
        "\n",
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2TcSrw4sTSX",
        "outputId": "179ad989-03c2-4e86-e004-f58289b99761"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import a Model**"
      ],
      "metadata": {
        "id": "RG-dLd-qsZ6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing that we are going to do is import a model. For this, we have saved all of our models on google drive so we will first mount our drive, then we will import the model given the file path.\n",
        "\n",
        "If the model's file path is not saved on drive, you will need to first upload the model's file from your local machine (disregard if you are running this locally)."
      ],
      "metadata": {
        "id": "q3IC_09JJ-NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jyKq6EMXNve0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, just go through the drive directory and find the file that you want to upload. In this case, we are uploading the model we are going to use for testing. When you have found it, right click the file and copy the path."
      ],
      "metadata": {
        "id": "BjhilIJKKWik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !!!! SET THIS TO MATCH YOUR DRIVE !!!!\n",
        "model_path = None\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HL7q305LyDRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a model, we can go ahead and prepare our dataset."
      ],
      "metadata": {
        "id": "8-xJ9ic_K_5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading A Dataset**"
      ],
      "metadata": {
        "id": "OlJweHPZMuVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can load whichever dataset that you would like. You either load the SynthDerm dataset, or the DDI dataset. **DO NOT RUN ALL OF THE BELOW CODE.** Only run section (a) or (b), depending on the desired dataset.\n",
        "\n",
        "Firstly, just set up the path for the dataset zip file below."
      ],
      "metadata": {
        "id": "sOXwrPCWMxZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !!!! CHANGE THIS PATH TO UNZIP THE CORRECT FILE !!!!\n",
        "ds_path = None"
      ],
      "metadata": {
        "id": "05R05xQ4NV6r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(a) Loading SynthDerm**"
      ],
      "metadata": {
        "id": "-uvQqJMRseK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we just want to download and unzip our dataset so that we can start setting it up. Since you have mounted your drive, the downloading is basically already done, so we can just unzip it.\n",
        "\n",
        "Also, make sure that you are using the SynthDermPrepared dataset and not the SynthDerm dataset. The only difference is that I pre-combined all of the metadata and added the Fitzpatrick skin type labels in the prepared dataset. The only thing that we will need to add to this dataset is the image path. So, we are going to read in our metadata into a dataframe and then just add an image path for each image."
      ],
      "metadata": {
        "id": "wFgavRKr4hRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip and delete unused folder\n",
        "!unzip \"$ds_path\" -d /content\n",
        "!rm -r /content/__MACOSX"
      ],
      "metadata": {
        "id": "ygzaMXmw47_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are just going to set some dataset specific variables that we are going to use now and later during testing."
      ],
      "metadata": {
        "id": "q34FlIczMkln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTO SET DATASET VARIABLES\n",
        "fitz_col = 'fitzpatrick_type'\n",
        "classes = ['Benign', 'Malignant']\n",
        "fitz_keys = [1, 2, 3, 4, 5, 6]"
      ],
      "metadata": {
        "id": "sy7-3gnufKYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can read in the metadata from the csv file and add a file path to each image in the metadata."
      ],
      "metadata": {
        "id": "A11rY5kEknMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the metadata from the csv file\n",
        "df = pd.read_csv('/content/SynthDermPrepared/SynthDerm_metadata.csv')\n",
        "\n",
        "# Get the paths for all images\n",
        "base_img_dir = '/content/SynthDermPrepared/images'\n",
        "df['path'] = df['image_id'].map(lambda img_id: base_img_dir + '/' + img_id)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PmV_ONHG8KW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our dataframe is all set up, so we can continue and use the dataframe to create our dataset object."
      ],
      "metadata": {
        "id": "9unB3_kjMrzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(b) Loading DDI**"
      ],
      "metadata": {
        "id": "qhGUp8pcxNVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code with load the dataframe for the DDI dataset. Since we are switching between a few different versions of this dataset, adjust the values as needed.\n",
        "\n",
        "All we are going to do below is:\n",
        "\n",
        "1. Unzip the dataset zip file\n",
        "2. Setup some variables that we will use to reference the specific dataset\n",
        "3. Read in the metadata and add image paths, labels, and label indexes.\n",
        "\n",
        "To start, let's unzip the dataset file."
      ],
      "metadata": {
        "id": "0awxtDylxbLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip and delete unused folder\n",
        "!unzip \"$ds_path\" -d /content\n",
        "!rm -r /content/__MACOSX"
      ],
      "metadata": {
        "id": "sUjj5OdlyJ-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's setup some dataset specific variables that we will use now and later in testing."
      ],
      "metadata": {
        "id": "b2p2U4ntL_e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the values for various dataset variables.\n",
        "fitz_col = 'skin_tone'\n",
        "classes = ['Benign', 'Malignant']\n",
        "fitz_keys = [12, 34, 56]"
      ],
      "metadata": {
        "id": "7DCZIcROzzsA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can read in the metadata into a dataframe and add our image paths, labels, and label indexes."
      ],
      "metadata": {
        "id": "PGVZlwUYMEUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the csv file\n",
        "metadata_filepath = '/content/ddi/updated_ddi_metadata.csv'\n",
        "df = pd.read_csv(metadata_filepath)\n",
        "\n",
        "# Setup the paths in the csv file\n",
        "base_img_dir = '/content/ddi/images'\n",
        "df['path'] = df['DDI_file'].map(lambda img_file: base_img_dir + '/' + img_file)\n",
        "\n",
        "# Setup the label and label_index\n",
        "df['label_ind'] = df['malignant'].map(lambda x: 1 if x else 0)\n",
        "df['label'] = df['label_ind'].map(lambda x: classes[x])\n",
        "\n",
        "# Drop the unused column\n",
        "df.pop('Unnamed: 0')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "DzIXs6woxaw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our dataframe is all set up, so we can continue and use the dataframe to create our dataset object."
      ],
      "metadata": {
        "id": "AIh3jNBGMKm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the Images Into A Dataset Object**"
      ],
      "metadata": {
        "id": "gnsUQwAUL83J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, one of the first things that we are going to do is get rid of any images that don't have a fitzpatrick skin type assigned."
      ],
      "metadata": {
        "id": "YkbHm4YHdWOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all data that doesn't have a fitzpatrick label\n",
        "df = df[df[fitz_col].notna()]"
      ],
      "metadata": {
        "id": "mmjTp5kRdgKQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the dataframe all set up, we can go ahead and get all of the paths and set up a Dataset object.\n",
        "\n",
        "Note, we don't need to add the labels to the dataset object directly as the model won't see them. We are just using the dataset object to feed in the images to get predictions that we can then line up with the labels in our dataframe. Though, it is really important that we **don't shuffle our dataset** at all in this process. Otherwise, when we add the model's predictions for each image back into the dataframe for analysis, the predictions would be invalid."
      ],
      "metadata": {
        "id": "ccrIetNCMCGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_img_size = (224, 224)  # matches the input size of the model\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Reads in an image given its file path and resizes it to new_img_size\n",
        "@tf.function\n",
        "def prepare_image(file_path):\n",
        "  image = tf.io.read_file(file_path)\n",
        "  image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "  image = tf.image.resize(image, new_img_size)\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "  return image\n",
        "\n",
        "# Extract the paths and labels and make a dataset object\n",
        "paths = df['path'].to_numpy()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
        "dataset = dataset.map(prepare_image, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Rupl8x5v8ziE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making and Processing Model Predictions**"
      ],
      "metadata": {
        "id": "RkEyTXoq_htB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model and dataset are setup, we can go ahead and use our model to get a prediction for each image."
      ],
      "metadata": {
        "id": "hSziwBLKNSqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, if you would like to use a GPU to speed things up, here is the code to set it up."
      ],
      "metadata": {
        "id": "nDymWy9iIack"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is accessible to TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Get the GPU memory fraction to allocate\n",
        "gpu_memory_fraction = 0.65\n",
        "\n",
        "# Create GPUOptions with the fraction of GPU memory to allocate\n",
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
        "\n",
        "# Create a session with the GPUOptions\n",
        "session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
      ],
      "metadata": {
        "id": "Q9hhg_3RIf5B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can go ahead and use the model to make predictions on the data."
      ],
      "metadata": {
        "id": "tQHxzg4EF9Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions of the test data\n",
        "test_pred = model.predict(dataset)\n",
        "\n",
        "# Save the continuous predictions in the dataframe\n",
        "df['cont_prediction'] = test_pred\n",
        "df.head()"
      ],
      "metadata": {
        "id": "F5Jna_cCF9vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are just going to interpret the predictions, depending on the model (binary classifier vs multi classifier), the number of classes in the dataset, and a specified threshold.\n",
        "\n",
        "The code below will allow us to interpret our multi-class models in comparison to our binary dataset. When we do this, we are taking any labels that are either melanoma (1) or carcinoma (2), and making them malignant labels. Any benign label (0) are kept as benign labels."
      ],
      "metadata": {
        "id": "dq-3aoSav9_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SET THE THRESHOLD FOR TESTING\n",
        "threshold = 0.5\n",
        "\n",
        "# Make a custom error to catch a mismatch between model and data classes\n",
        "class DsModelMismatchError(Exception):\n",
        "  \"CANNOT CONVERT BINARY CLASSIFICATION TO MULTI-CLASS CLASSIFICATION\"\n",
        "  pass\n",
        "\n",
        "# Determine the number of classes in the dataset and if the model is a binary classifier or not\n",
        "ds_classes = len(classes)\n",
        "binary_classifier_model = model.layers[-1].output_shape[1] <= 1\n",
        "\n",
        "# Convert the model predictions to match the dataset\n",
        "if ds_classes > 2 and not binary_classifier_model:\n",
        "  # DS and Model both multi-class\n",
        "  print(\"Both Dataset and Model are multi-class -- Computing argmax of output tensor\")\n",
        "  df['prediction'] = np.argmax(test_pred, axis=-1)\n",
        "elif ds_classes == 2 and binary_classifier_model:\n",
        "  # DS and Model both Binary\n",
        "  print(\"Both Dataset and Model are Binary Classification -- \", end='')\n",
        "  # Apply sigmoid activation if it has not been done and round with threshold = 0.5\n",
        "  if (max(test_pred) <= 1) and (min(test_pred) >= 0):\n",
        "    #test_pred = [np.array(x).round().astype(int).item() for x in test_pred]\n",
        "    df['prediction'] = df['cont_prediction'].map(lambda x: 1 if x > threshold else 0)\n",
        "    print(\"No Activation Done - All Values in [0, 1] - Rounding with Threshold = \" + str(threshold))\n",
        "  else:\n",
        "    df['prediction'] = df['cont_prediction'].map(lambda x: tf.keras.activations.sigmoid(x))\n",
        "    df['prediction'] = df['prediction'].map(lambda x: 1 if x > threshold else 0)\n",
        "    print(\"Sigmoid Activation Done - Rounding with Threshold = \" + str(threshold))\n",
        "elif ds_classes == 2 and not binary_classifier_model:\n",
        "  # Convert multi-class predictions to binary classification\n",
        "  print(\"Converting Multi-Class Classifications into Binary Classifications\")\n",
        "  test_pred = np.argmax(test_pred, axis=-1)\n",
        "  df['prediction'] = np.array([0 if x == 0 else 1 for x in test_pred])\n",
        "else:\n",
        "  raise DsModelMismatchError\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "35ncW3Q3oLad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have each image paired with its metadata, its continuous prediction, and its discrete prediction. So, we can start analyzing how the model did."
      ],
      "metadata": {
        "id": "ilg9XAUy1Hy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyzing The Predictions**"
      ],
      "metadata": {
        "id": "YyWfrqtTtj7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the predictions lined up with the metadata, we can start analyzing how the model did over the dataset."
      ],
      "metadata": {
        "id": "Wt2tbk9uL8eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly though, if you want to save the dataframe with the predictions included so that you can come back to this without having to run all of the above code, run the below cell. Just make sure to give it a good filename, and download it to your local storage afterwards."
      ],
      "metadata": {
        "id": "20qtAOweMFPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the output filename for the csv file\n",
        "csv_output_filename = None\n",
        "\n",
        "# SAVE THE METADATA WITH PREDICTIONS TO A CSV FILE\n",
        "df.to_csv(csv_output_filename)"
      ],
      "metadata": {
        "id": "D99w0xwlMTF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, use the code below to load up a dataframe from a csv file, if you saved it from the code above. Just make sure to go back and run the install and imports, as well as any cells when making the dataset that create variables specific to that dataset (eg \"fitz_col\")"
      ],
      "metadata": {
        "id": "i3Mz02jQITvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nnASYZTZIfg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if you only want to look at certain parts of the dataset in our testing, run the following cells below. For example, if you only want to look at the images in the testing subset or images that have \"common\" diagnoses.\n",
        "\n",
        "**If you wish to keep all images in the testing, DO NOT RUN THE BELOW TWO CELLS**"
      ],
      "metadata": {
        "id": "ZKziG8go17IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all data that are not a part of the test set\n",
        "df = df[df['Set'] == 'Test']\n",
        "print(\"WARNING -- REMOVING ALL TRAINING SET IMAGES FROM DATA -- WARNING\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hKu_Wstnydcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all data that are not considered to have a common diagnoses\n",
        "df = df[df['common'] == True]\n",
        "print(\"WARNING -- REMOVING ALL UNCOMMON IMAGES FROM DATA -- WARNING\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ia2aONMGGmGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing The Model With The Entire Dataset**"
      ],
      "metadata": {
        "id": "Ei5Brz9j0Ao3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start to analyze how the model did against each aggregated skin type, let's first see how well it did against the dataset as a whole.\n",
        "\n",
        "Firstly, let's create a classification report."
      ],
      "metadata": {
        "id": "pDQ3Hqif0GhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['label_ind'].to_numpy()\n",
        "test_pred = df['prediction'].to_numpy()\n",
        "print(classification_report(labels, test_pred))"
      ],
      "metadata": {
        "id": "LOJBopRiG-dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's analyze the ROC-AUC for the entire dataset."
      ],
      "metadata": {
        "id": "nOcYHZij1QGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the title for the graph\n",
        "ds = 'All DDI Images'\n",
        "title = 'ROC Curve - ' + ds\n",
        "\n",
        "# Get the ROC-AUC data from the test results\n",
        "fpr, tpr, threshold = roc_curve(labels, df['cont_prediction'].to_numpy())\n",
        "auc_keras = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC-AUC curve\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='Model (area = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title(title)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYizqYvx11Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's create a confusion matrix to analyze how the model on the overall dataset."
      ],
      "metadata": {
        "id": "7yl23w_E3_Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the confusion matrix\n",
        "labels = df['label_ind'].to_numpy()\n",
        "preds = df['prediction'].to_numpy()\n",
        "cm = sklearn.metrics.confusion_matrix(labels, preds)\n",
        "\n",
        "# Plot the consuion matrix\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix - Entire Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IhxXmDSI4FyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, while we have found this data before, let's go ahead and print the distribution of labels in the dataset we are using."
      ],
      "metadata": {
        "id": "ga5bcBhU0cCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_to_chart = 'label'\n",
        "\n",
        "# Plot the distribution with its percentage.\n",
        "fig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\n",
        "ax = df[category_to_chart].value_counts().plot(kind='bar', ax=ax1, title='Test Dataset Label Distribution',\n",
        "                                               xlabel='Class', ylabel='Count')\n",
        "ax.bar_label(ax.containers[0])\n",
        "\n",
        "counts = df[category_to_chart].value_counts()\n",
        "percents = counts.map(lambda x: (x / counts.sum()) * 100.0)\n",
        "print(\"Percentages: \")\n",
        "print(percents)"
      ],
      "metadata": {
        "id": "RPa6PpRW0gPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing Across Fitzpatrick Skin Type**"
      ],
      "metadata": {
        "id": "xAfNhFEY00nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have looked at how the model did overall, let's look at how the model did against each Fitzpatrick skin type. Here, we will be computing and comparing the accuracy, the ROC-AUC, and the confusion matrices."
      ],
      "metadata": {
        "id": "-OQvyDHj4rv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Fitzpatrick Skin Type Distribution of the Dataset"
      ],
      "metadata": {
        "id": "hVttJIvcnZgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can see how the model did for certain skin types.\n",
        "\n",
        "First though, let's just see the distribution of the dataset."
      ],
      "metadata": {
        "id": "RRtDctGX06Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE THIS TO PLOT DIFFERENT CATEGORIES\n",
        "category_to_chart = fitz_col\n",
        "\n",
        "# Plot the distribution with its percentage.\n",
        "fig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\n",
        "ax = df[category_to_chart].value_counts().sort_index().plot(kind='bar', ax=ax1,\n",
        "                                                            title='Test Dataset Fitzpatrick Skin Type Distribution',\n",
        "                                                            xlabel='Fitzpatrick Skin Type',\n",
        "                                                            ylabel='Number of Images')\n",
        "ax.bar_label(ax.containers[0])\n",
        "\n",
        "counts = df[category_to_chart].value_counts().sort_index()\n",
        "percents = counts.map(lambda x: (x / counts.sum()) * 100.0)\n",
        "print(\"Percentage of Dataset: \")\n",
        "print(percents)"
      ],
      "metadata": {
        "id": "mxcilAjl4VTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see the distribution of skin type with information about the classes as well.\n",
        "\n",
        "Firstly, we're going to define a helper function that will fill in any missing values from our value counts.\n",
        "\n",
        "Then, we can go ahead and get the aggregated counts and plot them."
      ],
      "metadata": {
        "id": "qoxIz3LzoemZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fills in the missing indexes in the Series 'srs' (must have all values specified in fitz_keys)\n",
        "# with the value val\n",
        "def fill_missing(srs, val=0):\n",
        "  srs_dict = srs.to_dict()\n",
        "  for key in fitz_keys:\n",
        "    if key not in srs_dict:\n",
        "      srs_dict[key] = 0\n",
        "  return pd.Series(srs_dict).sort_index()\n",
        "\n",
        "# Get counts for the distribution for each skin type and class\n",
        "agg_dist_counts = []\n",
        "for cat in classes:\n",
        "  # Extract all info for a given class\n",
        "  sub_df = df.loc[df['label'] == cat]\n",
        "  # Get counts for each skin type and fill in the missing counts\n",
        "  skin_type_count = sub_df[fitz_col].value_counts().sort_index()\n",
        "  skin_type_count = fill_missing(skin_type_count)\n",
        "  # Add the counts and class (cat) to the list\n",
        "  agg_dist_counts.append((cat, np.array(skin_type_count)))"
      ],
      "metadata": {
        "id": "ZvAs04AepUlV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can plot the distribution that we calculated above."
      ],
      "metadata": {
        "id": "caf7L__2SFni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution across each individual class\n",
        "x = np.arange(1, len(fitz_keys) + 1)\n",
        "width = 0.8 / len(classes)  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 10), layout='constrained')\n",
        "\n",
        "for class_name, values in agg_dist_counts:\n",
        "  offset = width * multiplier\n",
        "  rects = ax.bar(x + offset, values, width, label=class_name)\n",
        "  ax.bar_label(rects, padding=3, fontsize='large')\n",
        "  multiplier += 1\n",
        "\n",
        "ax.set_xlabel('Fitzpatrick Skin Type', fontsize='large')\n",
        "ax.set_ylabel('Count', fontsize='large')\n",
        "ax.set_title('Fitzpatrick Skin Type Class Distribution')\n",
        "xtick_offset = width if len(classes) > 2 else (0.5 * width)\n",
        "ax.set_xticks(x + xtick_offset, fitz_keys)\n",
        "ax.legend(loc='upper right', fontsize='xx-large')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O9AdWV9SoeFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing The Model's Accuracy For Each Skin Type Across All Classes"
      ],
      "metadata": {
        "id": "WnQw54x1ng49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen the distribution in the test set, we can extract the information that we want from the DataFrame. All that is getting done here is we are extracting all of the information for a certain skin type, and then we are extracting from that only the cases where the image was classified correctly. Then we can compare the number of samples in each set and determine an accuracy for that skin type."
      ],
      "metadata": {
        "id": "icGXHpCy6CKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_acc = []\n",
        "class_agg_acc = [[], [], []]\n",
        "\n",
        "\n",
        "# Get a list of the different values for fitz_col\n",
        "f_types = np.unique(df[fitz_col].to_numpy())\n",
        "\n",
        "# Get the count of correct predictions for each skin type\n",
        "for f_type in f_types:\n",
        "  # Extract the label and prediction for all data of current f_type, then repeat for only correct predictions\n",
        "  sub_df = df.loc[df[fitz_col] == f_type][['label_ind', 'prediction', 'label']]\n",
        "  sub_correct_df = sub_df.loc[sub_df['label_ind'] == sub_df['prediction']]\n",
        "\n",
        "  # Calculate the overall accuracy\n",
        "  num_correct = len(sub_correct_df)\n",
        "  num_total = len(sub_df)\n",
        "  agg_acc.append((num_correct, num_total))\n",
        "\n",
        "  # Calculate the accuracy for the curr skin type and each label\n",
        "  for i, cat in enumerate(classes):\n",
        "    class_sub_df = sub_df.loc[sub_df['label'] == cat]\n",
        "    class_corr_df = sub_correct_df.loc[sub_correct_df['label'] == cat]\n",
        "    num_correct = len(class_corr_df)\n",
        "    num_total = len(class_sub_df)\n",
        "    class_agg_acc[i].append((num_correct, num_total))\n",
        "\n",
        "# If the last list of class_agg_acc is empty, delete it\n",
        "if len(class_agg_acc[-1]) == 0:\n",
        "    class_agg_acc.pop(-1)\n"
      ],
      "metadata": {
        "id": "PGfPK8btHB8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a list of all of the counts of number of correct predictions with the total number of images for that type and class, we can process them to see how accurate the model was on our dataset.\n",
        "\n",
        "Firstly, we will just define a function that will anaylyze one of the lists containing either 3 or 6 (depending on if skin type is already aggregated or not) tuples of (num_correct, num_total). Then, we can use it to analyze the results."
      ],
      "metadata": {
        "id": "XAJbeLQz7E65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processes a list of tuples containing (num_correct, num_total), corresponding\n",
        "# to the list of skin types (fitz_types) which might or might not be aggregated.\n",
        "def process_accuracies(acc_list, fitz_types, classification=None, precision=2):\n",
        "  # Create a DataFrame for the agg_acc\n",
        "  agg_acc_df = pd.DataFrame(acc_list, columns=['Num_Correct', 'Num_Total'])\n",
        "  agg_acc_df['Accuracy'] = (agg_acc_df['Num_Correct'] / agg_acc_df['Num_Total']) * 100\n",
        "  agg_acc_df['Fitzpatrick'] = fitz_types;\n",
        "  agg_acc_df = agg_acc_df[['Fitzpatrick', 'Accuracy', 'Num_Correct', 'Num_Total']]\n",
        "\n",
        "  # Aggregate the data to combine Fitzpatrick skin types (1&2, 3&4, 5&6) - if not already done\n",
        "  if len(acc_list) >= 6:       # if data not already aggregated\n",
        "    # Mark existing data as not aggregated\n",
        "    agg_acc_df['Aggregated'] = False\n",
        "    # Aggregate the values\n",
        "    comb = []\n",
        "    for i in range(0, 6, 2):\n",
        "      num_corr = acc_list[i][0] + acc_list[i+1][0]\n",
        "      num_tot = acc_list[i][1] + acc_list[i+1][1]\n",
        "      comb.append((num_corr, num_tot))\n",
        "    comb_df = pd.DataFrame(comb, columns=['Num_Correct', 'Num_Total'])\n",
        "    comb_df['Accuracy'] = (comb_df['Num_Correct'] / comb_df['Num_Total']) * 100\n",
        "    comb_df['Fitzpatrick'] = ['1&2', '3&4', '5&6']\n",
        "    comb_df = comb_df[['Fitzpatrick', 'Accuracy', 'Num_Correct', 'Num_Total']]\n",
        "    comb_df['Aggregated'] = True\n",
        "    # Combine the non-aggregated df and aggregated df\n",
        "    agg_acc_df = pd.concat([agg_acc_df, comb_df])\n",
        "  else:\n",
        "    agg_acc_df['Aggregated'] = True\n",
        "\n",
        "  # If the class is specified, add it to the dataframe\n",
        "  if classification != None:\n",
        "    agg_acc_df['Class'] = classification\n",
        "    agg_acc_df = agg_acc_df[['Fitzpatrick', 'Class', 'Accuracy', 'Num_Correct', 'Num_Total', 'Aggregated']]\n",
        "\n",
        "  # Round the values of accuracy to the specified precision\n",
        "  agg_acc_df['Accuracy'] = agg_acc_df['Accuracy'].map(lambda acc: round(acc, precision))\n",
        "\n",
        "  return agg_acc_df"
      ],
      "metadata": {
        "id": "uPLRW_ZqXtlK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, let's just analyze how the dataset did in terms of accuracy for each skin type, in terms of all classes. What is being measured here is the classification accuracy for each skin type.\n",
        "\n",
        "\n",
        "$type\\_n\\_accuracy = \\dfrac{num\\_type\\_n\\_correct}{num\\_type\\_n\\_total} * 100$\n"
      ],
      "metadata": {
        "id": "IkwU8oPAjVDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data into a dataframe\n",
        "agg_acc_df = process_accuracies(agg_acc, f_types)\n",
        "\n",
        "# Print a report for accuracy within each skin type\n",
        "print(\"------ Accuracy Report on All Classes Per Fitzpatrick Skin Type ------\")\n",
        "agg_acc_df"
      ],
      "metadata": {
        "id": "RuYxhoyj7Ed_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the Model's Accuracy For Each Skin Type Across Each Class"
      ],
      "metadata": {
        "id": "0ECShjKKTQ6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can start analyzing the data in terms of both Skin Type and the class itself. This code will give you an accuracy for each skin type and class. If you see \"NaN\" for an accuracy term, that means that there was not any images for that specific type and class.\n",
        "\n",
        "Just to be clear, here accuracy is defined as:\n",
        "\n",
        "$type\\_n\\_class\\_c\\_accuracy = \\dfrac{num\\_type\\_n\\_class\\_c\\_correct}{num\\_type\\_n\\_class\\_c\\_total} * 100$"
      ],
      "metadata": {
        "id": "NciQAkJzk8vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the Dataframe\n",
        "class_agg_acc_df_list = [process_accuracies(class_agg_acc[i], f_types, cat) for i, cat in enumerate(classes)]\n",
        "class_agg_acc_df = pd.concat(class_agg_acc_df_list)\n",
        "\n",
        "# Print a report for accuracy within each skin type for each classification\n",
        "print(\"------ Accuracy Report on Per Class and Per Fitzpatrick Skin Type ------\")\n",
        "class_agg_acc_df"
      ],
      "metadata": {
        "id": "4MTm-Gg4_ui_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can visualize this data in a bar chart. Firstly, we will just process the data into a more usable form."
      ],
      "metadata": {
        "id": "qFAraw9kThMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the accuracy information from the dataframe\n",
        "acc_list = []        # Store accuracies of type (1, 2, 3, 4, 5, 6)\n",
        "acc_agg_list = []    # Store accuracies of type (1&2, 3&4, 5&6)\n",
        "for class_name in classes:\n",
        "  sub_df = class_agg_acc_df.loc[class_agg_acc_df['Class'] == class_name][['Aggregated', 'Accuracy']]\n",
        "  sub1_df = sub_df.loc[sub_df['Aggregated'] == False]\n",
        "  sub2_df = sub_df.loc[sub_df['Aggregated'] == True]\n",
        "\n",
        "  acc_list.append((class_name, sub1_df['Accuracy'].to_numpy()))\n",
        "  acc_agg_list.append((class_name, sub2_df['Accuracy'].to_numpy()))"
      ],
      "metadata": {
        "id": "7FuwLBAHTj5q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets plot the accuracies for the non-aggregated data. Note, if you are using the DDI dataset, this data will not exist, and so it will print a warning. The below cell will only run for the SynthDerm dataset.\n",
        "\n",
        "Note: If a bar in the chart says 0, then it had 0% accuracy. However, if no value is there, there was not any images in the dataset for that specific area, so no accuracy value exists there."
      ],
      "metadata": {
        "id": "eHxIGfE4YDpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(f_types) < 6:\n",
        "  print('WARNING -- CANNOT PLOT ACCURACIES FOR NON-AGGREGATED DATA -- DATA DOES NOT EXISTS')\n",
        "else:\n",
        "  # Plot the accuracy across each individual class and skin type\n",
        "  x = np.arange(1,7)\n",
        "  width = 0.8 / 6  # the width of the bars\n",
        "  multiplier = 0\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(15, 10), layout='constrained')\n",
        "\n",
        "  for class_name, values in acc_list:\n",
        "    offset = width * multiplier\n",
        "    rects = ax.bar(x + offset, values, width, label=class_name)\n",
        "    ax.bar_label(rects, padding=3, fontsize='large')\n",
        "    multiplier += 1\n",
        "\n",
        "  ax.set_xlabel('Fitzpatrick Skin Type', fontsize='large')\n",
        "  ax.set_ylabel('Model Accuracy', fontsize='large')\n",
        "  ax.set_title('Fitzpatrick Skin Type Class Accuracies')\n",
        "  xtick_offset = width if len(classes) > 2 else (0.5 * width)\n",
        "  ax.set_xticks(x + xtick_offset, (1, 2, 3, 4, 5, 6))\n",
        "  ax.legend(loc='upper left', fontsize='xx-large')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "w0HHK-scV3PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the aggregated data"
      ],
      "metadata": {
        "id": "ZSPc8T06YJqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy across each individual class and skin type\n",
        "x = np.arange(1,4)\n",
        "width = 0.8 / 6  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10), layout='constrained')\n",
        "\n",
        "for class_name, values in acc_agg_list:\n",
        "  offset = width * multiplier\n",
        "  rects = ax.bar(x + offset, values, width, label=class_name)\n",
        "  ax.bar_label(rects, padding=3, fontsize='large')\n",
        "  multiplier += 1\n",
        "\n",
        "ax.set_xlabel('Fitzpatrick Skin Type', fontsize='large')\n",
        "ax.set_ylabel('Model Accuracy', fontsize='large')\n",
        "ax.set_title('Fitzpatrick Skin Type Class Accuracies')\n",
        "xtick_offset = width if len(classes) > 2 else (0.5 * width)\n",
        "ax.set_xticks(x + xtick_offset, ('1&2', '3&4', '5&6'))\n",
        "ax.legend(loc='upper right', fontsize='small')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LrHthL_PYJaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC-AUC for Binary Classification"
      ],
      "metadata": {
        "id": "_lntNVdssjUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since ROC-AUC is mainly for binary classification, we are only going to continue here for our binary classification models (which we are mainly using for our final models anyways)."
      ],
      "metadata": {
        "id": "4vA91FFAsywc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will just split the data up into the different skin types, aggregating it if specified, and calculating all of the roc-auc data. If you want to aggregate the skin types (go from {1, 2, ..., 6} to {1&2, 3&4, 5&6}), just change the value of aggregate_f_type."
      ],
      "metadata": {
        "id": "tXKHlgGixg4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !!!! SET THIS TO AGGREGATE OR NOT AGGREGATE THE RESULTS !!!!\n",
        "aggregate_f_type = False\n",
        "f_types = np.unique(df[fitz_col].to_numpy())\n",
        "\n",
        "# Extract labels and predictions from df and calculate fpr, tpr, and auc for\n",
        "# each skin type, with/without aggregating skin types.\n",
        "def get_roc_data(df, aggregate, f_types):\n",
        "  temp_fitz_col = fitz_col\n",
        "\n",
        "  # Aggregate the skin types if needed\n",
        "  if aggregate and len(f_types) == 6:\n",
        "    fitz_agg_dict = {\n",
        "        1: '1&2',\n",
        "        2: '1&2',\n",
        "        3: '3&4',\n",
        "        4: '3&4',\n",
        "        5: '5&6',\n",
        "        6: '5&6'\n",
        "    }\n",
        "    temp_fitz_col = 'agg_fitz'\n",
        "    df[temp_fitz_col] = df[fitz_col].map(fitz_agg_dict.get)\n",
        "    f_types = np.unique(df[temp_fitz_col].to_numpy())\n",
        "\n",
        "  # For each skin type, get the fpr, tpr, and auc\n",
        "  roc_data = []\n",
        "  f_types = np.unique(df[temp_fitz_col].to_numpy())\n",
        "  for f_type in f_types:\n",
        "    sub_df = df[df[fitz_col] == f_type]\n",
        "    labels = sub_df['label_ind'].to_numpy()\n",
        "    preds = sub_df['cont_prediction'].to_numpy()\n",
        "    try:\n",
        "      fpr, tpr, threshold = roc_curve(labels, preds)\n",
        "      auc_val = auc(fpr, tpr)\n",
        "      roc_data.append((fpr, tpr, auc_val, f_type))\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return roc_data\n",
        "\n",
        "roc_data = get_roc_data(df, aggregate_f_type, f_types)"
      ],
      "metadata": {
        "id": "eQXS_x7st44b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have calculated all of our ROC-AUC data, we can go ahead an graph it."
      ],
      "metadata": {
        "id": "fuvW2vqr7Ki_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the name of the dataset used for the graph title\n",
        "ds = 'All DDI Images'\n",
        "title = 'ROC curve - ' + ds + ' - Agg. Fitzpatrick Skin Type'\n",
        "\n",
        "# Plot the ROC-AUC graphs\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "for fpr, tpr, auc_val, fitz_type in roc_data:\n",
        "  plt.plot(fpr, tpr, label=f'Fitz Type: {fitz_type} (area = {auc_val:.3f})')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title(title)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JWF7Cf4mxzUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the Confusion Matrices, F1 Scores, and Classification Reports For Each Skin Type"
      ],
      "metadata": {
        "id": "WaTSsYV-WYox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we are going to generate a confusion matrix for each skin type. This way, we can compare things like precision and recall for different skin types."
      ],
      "metadata": {
        "id": "tEKuMvTq7hBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below is just a function which will generate the confusion matrix itself. Below that, we will actually generate each of the plots."
      ],
      "metadata": {
        "id": "Yk0sm0_SWcSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a confusion matrix\n",
        "def get_agg_confusion_matrix(df, fitz_type=None):\n",
        "  '''\n",
        "  Returns a sklearn confusion matrix object for a specific skin type.\n",
        "\n",
        "  Parameters:\n",
        "    df - DataFrame - The dataframe with all metadata, labels, and predictions\n",
        "\n",
        "    fitz_type - int - The fitzpatrick skin type to return the aggregated information of.\n",
        "                      If None, returns all skin type information\n",
        "\n",
        "  Returns:\n",
        "    A sklearn confusion matrix object.\n",
        "  '''\n",
        "  # Filter the data based on the skin type\n",
        "  sub_df = df\n",
        "  if fitz_type:\n",
        "    sub_df = sub_df[sub_df[fitz_col] == fitz_type]\n",
        "\n",
        "  # Create the confusion matrix\n",
        "  labels = sub_df['label_ind'].to_numpy()\n",
        "  preds = sub_df['prediction'].to_numpy()\n",
        "  cm = sklearn.metrics.confusion_matrix(labels, preds)\n",
        "  return cm"
      ],
      "metadata": {
        "id": "-z5TNI9rWbtc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use the above function to create and plot each confusion matrix."
      ],
      "metadata": {
        "id": "ad8QYQQu8MV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all available fitzpatrick skin types\n",
        "fitz_types = list(np.unique(df[fitz_col].to_numpy()))\n",
        "\n",
        "# For each skin type, generate and plot the confusion matrix\n",
        "for i, fitz_type in enumerate(fitz_types):\n",
        "  cm = get_agg_confusion_matrix(df, fitz_type)\n",
        "  disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "  disp.plot()\n",
        "  plt.title('Confusion Matrix - FST (Orig): ' + str(fitz_type))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "49FTivHUYal9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can do basically the same thing as we did above, but just calculate the F1 scores instead.\n",
        "\n",
        "The below function will simply calculate the F1 score for a specified skin type."
      ],
      "metadata": {
        "id": "k6ySSUUHfaNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_agg_F1_score(df, fitz_type=None):\n",
        "  '''\n",
        "  Returns the F1 score for a skin type.\n",
        "\n",
        "  Parameters:\n",
        "    df - DataFrame - The dataframe with all metadata, labels, and predictions\n",
        "\n",
        "    fitz_type - int - The fitzpatrick skin type to return the aggregated information of.\n",
        "                      If None, returns all skin type information\n",
        "\n",
        "  Returns:\n",
        "    The F1 score.\n",
        "  '''\n",
        "  # Filter the data based on the skin type\n",
        "  sub_df = df\n",
        "  if fitz_type:\n",
        "    sub_df = sub_df[sub_df[fitz_col] == fitz_type]\n",
        "\n",
        "  # Create the confusion matrix\n",
        "  labels = sub_df['label_ind'].to_numpy()\n",
        "  preds = sub_df['prediction'].to_numpy()\n",
        "  f1_score = sklearn.metrics.f1_score(labels, preds)\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "lpK3qklHejsC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calculate and print out each F1 score"
      ],
      "metadata": {
        "id": "m5-Dyfsh8wlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the precision to round F1 scores to\n",
        "prec = 2\n",
        "\n",
        "# Calculate and print each F1 score\n",
        "for fitz_type in fitz_types:\n",
        "  f1 = get_agg_F1_score(df, fitz_type)\n",
        "  f1 = round(f1, prec)\n",
        "  print('F1 Score - (Orig) FST ' + str(fitz_type) + ': ' + str(f1))"
      ],
      "metadata": {
        "id": "8Q9cZEe1fS9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will repeat this process for the classification reports."
      ],
      "metadata": {
        "id": "HCgsDecePlxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_agg_classification_report(df, fitz_type=None):\n",
        "  '''\n",
        "  Returns the classification report for a skin type.\n",
        "\n",
        "  Parameters:\n",
        "    df - DataFrame - The dataframe with all metadata, labels, and predictions\n",
        "\n",
        "    fitz_type - int - The fitzpatrick skin type to return the aggregated information of.\n",
        "                      If None, returns all skin type information\n",
        "\n",
        "  Returns:\n",
        "    The classification report.\n",
        "  '''\n",
        "  # Filter the data based on the skin type\n",
        "  sub_df = df\n",
        "  if fitz_type:\n",
        "    sub_df = sub_df[sub_df[fitz_col] == fitz_type]\n",
        "\n",
        "  # Create the confusion matrix\n",
        "  labels = sub_df['label_ind'].to_numpy()\n",
        "  preds = sub_df['prediction'].to_numpy()\n",
        "  f1_score = sklearn.metrics.classification_report(labels, preds)\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "EbTz4K5KPuvA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will generate and print the classification report for each skin type."
      ],
      "metadata": {
        "id": "XV_HRQ_89D0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print the classification reports for each skin type\n",
        "for fitz_type in fitz_types:\n",
        "  cf = get_agg_classification_report(df, fitz_type)\n",
        "  print('FST' + str(fitz_type) + ': ')\n",
        "  print(cf, end='\\n\\n')"
      ],
      "metadata": {
        "id": "OS84PxuzPq34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visually Examining Model Accuracy**"
      ],
      "metadata": {
        "id": "ovRpDiTswZbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we discussed in our meeting, one of the last things that we are going to do in analyzing our model, is to visually analyze the pictures to see which ones are classified correctly, and which are not.\n",
        "\n",
        "To do this, we are going to print out different subsets of images and see if we can find a pattern among them (ex: all images with hair are incorrectly classified)."
      ],
      "metadata": {
        "id": "MQvqDMob6DAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we're just going to add some shortened names to the dataframe that will help out when printing this data."
      ],
      "metadata": {
        "id": "dCPpz7ykwg9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shorten_dict = {\n",
        "    'Benign': 'Ben',\n",
        "    'Malignant': 'Mal'\n",
        "}\n",
        "\n",
        "pred_dict = {\n",
        "    0: 'Ben',\n",
        "    1: 'Mal'\n",
        "}\n",
        "\n",
        "df['short_label'] = df['label'].map(shorten_dict.get)\n",
        "df['short_pred_label'] = df['prediction'].map(pred_dict.get)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dW1Gz0dD1XtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the below function will return a filtered version of the provided dataframe. We can use this to analyze different subsets of our dataset."
      ],
      "metadata": {
        "id": "O-ZZ0bk99wO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame subset given the specified parameters\n",
        "def get_df_subset(_df, label_inds=None, fitz=None, correct=None, fp=False, fn=False):\n",
        "  '''\n",
        "  Returns a new filtered dataframe, given the specified parameters.\n",
        "\n",
        "  Parameters:\n",
        "    _df - DataFrame - The dataframe containing the image, and all associated metadata\n",
        "\n",
        "    label_inds - List[int] - A list of integer labels to return. If none, all labels are returned\n",
        "\n",
        "    fitz - List[] - A list of strings or integers which match the fitzpatrick labels. If none, all\n",
        "                    fitzpatrick types are returned\n",
        "\n",
        "    correct - bool - If true, only return images predicted correctly. If false, only return images not\n",
        "                     predicted correctly. If None, no filtering occurs.\n",
        "                     Only set to true if fp and fn are not set. If set to False (and fp and fn == false)\n",
        "                     both false positives and false negatives are returned.\n",
        "\n",
        "    fp - bool - If True, only return images that had false positive classifications. If False, no filtering\n",
        "                occurs.\n",
        "                Only set to true if correct and fn are not set.\n",
        "\n",
        "    fn - bool - If True only return images that had false negative classifications. If False, no filtering\n",
        "                occurs.\n",
        "                Only set to true if correct and fp are not set.\n",
        "\n",
        "  Returns:\n",
        "    df - DataFrame - A filtered version of the original Dataframe\n",
        "  '''\n",
        "  assert len(_df) > 0, \"ERROR - DataFrame does not have any tuples\"\n",
        "  df = _df\n",
        "\n",
        "  # Sort label indexes\n",
        "  if label_inds:\n",
        "    df = df[df['label_ind'].isin(label_inds)]\n",
        "\n",
        "  # Sort Fitzpatrick Types\n",
        "  if fitz:\n",
        "    df = df[df[fitz_col].isin(fitz)]\n",
        "\n",
        "  # Sort correct classifications\n",
        "  if correct == True:\n",
        "    assert (fp == False) and (fn == False), \"ERROR - Cannot filter false positives, false negativesn and true positives/negatives at the same time\"\n",
        "    df = df[df['label_ind'] == df['prediction']]\n",
        "  elif correct == False:\n",
        "    df = df[df['label_ind'] != df['prediction']]\n",
        "\n",
        "  # Sort false positives\n",
        "  if fp:\n",
        "    assert (not correct) and (fn == False), \"ERROR - Cannot filter false positives, false negativesn and true positives/negatives at the same time\"\n",
        "    df = df[df['label_ind'] == 0]\n",
        "    df = df[df['prediction'] == 1]\n",
        "\n",
        "  # Sort false negatives\n",
        "  if fn:\n",
        "    assert (not correct) and (fp == False), \"ERROR - Cannot filter false positives, false negativesn and true positives/negatives at the same time\"\n",
        "    df = df[df['label_ind'] == 1]\n",
        "    df = df[df['prediction'] == 0]\n",
        "\n",
        "  print(f\"Original DataFrame Size: {len(_df)}\\tNew DataFrame Size: {len(df)}\")\n",
        "  return df"
      ],
      "metadata": {
        "id": "I5SoOQ3QSdIM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can define a function that will print a series of images to the console. We will use this in order to print the results from the function above."
      ],
      "metadata": {
        "id": "2uhT7zKM99wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a list of images with their label, prediction, id, and fitzpatrick info\n",
        "def print_images(paths, labels, predictions, ids, fitz):\n",
        "  num_rows = int(len(paths) / 5) + 1\n",
        "  fig_len = num_rows * 5\n",
        "  fig = plt.figure(figsize=(15, fig_len))\n",
        "  fig.tight_layout()\n",
        "\n",
        "  new_img_size = (224, 224)\n",
        "  print(\"LEGEND: LABEL PREDICTION\")\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    ax = fig.add_subplot(num_rows, 5, i + 1)\n",
        "    img = tf.io.read_file(paths[i])\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, new_img_size)\n",
        "    img = tf.cast(img, 'int64')\n",
        "    ax.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(str(labels[i]) + \" \" + str(preds[i]))\n",
        "    plt.annotate(ids[i], (0.01, 0))\n",
        "    plt.annotate(fitz[i], (200, 0.01))"
      ],
      "metadata": {
        "id": "xI-eHU0C0HUk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally print out a bunch of images to visually examine how our model is doing. This is handled automatically. All that you will need to do is specify what kind of images you would like to print out. Read the documentation for the get_df_subset function to get more information on how to use it to print the images that you want. After that function is run and you get a new subset of images, you will run the print_images function to actually print them out.\n",
        "\n",
        "\n",
        "For each image, it will print out 4 bits of information.\n",
        "\n",
        "The biggest letters are the the label and prediction. In this order:\n",
        "\n",
        "  LABEL PREDICTION\n",
        "\n",
        "  Ex: Mal Mal\n",
        "\n",
        "The small numbers on the top left of the images are the ids, so that you can pick them out if you want to analyze them in more depth.\n",
        "\n",
        "The small numbers on the top right are the Fitzpatrick skin type information for that image."
      ],
      "metadata": {
        "id": "sdz44-z7-AKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this section, change the parameters to the get_df_subset function in order to analyze different parts of the dataset.\n",
        "\n",
        "As a warning, because we are printing out so many images, a large subset of images might take a while to print out."
      ],
      "metadata": {
        "id": "mSe6hoaT-kv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a subset of images from the original dataset\n",
        "subset_df = get_df_subset(df, fitz=[56])\n",
        "\n",
        "# !!!! DO NOT CHANGE THE BELOW CODE !!!! #\n",
        "# Extract the information necessary for printing\n",
        "paths = subset_df['path'].to_numpy()\n",
        "labels = subset_df['short_label'].to_numpy()\n",
        "preds = subset_df['short_pred_label'].to_numpy()\n",
        "ids = subset_df['DDI_ID'].to_numpy()\n",
        "skin_tones = subset_df['skin_tone'].to_numpy()\n",
        "\n",
        "# Print the subset of images\n",
        "print_images(paths, labels, preds, ids, skin_tones)"
      ],
      "metadata": {
        "id": "8Bn45YF9bxg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}